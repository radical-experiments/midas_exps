{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling for alchemlyb example\n",
    "The purpose of this notebook is to understand performance bottleneck in a particular use case by profiling python functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benzene-in-water example for parsing\n",
    "\n",
    "This example was described during the presentation and there is a similar test script using TI at https://github.com/alchemistry/alchemlyb/blob/master/src/alchemlyb/tests/test_ti_estimators.py\n",
    "\n",
    "The highlighted point to investigate is about parsing multiple simulation files, and the specific function to run with the profiler is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmx_benzene_coul_dHdl():\n",
    "    dataset = alchemtest.gmx.load_benzene()\n",
    "\n",
    "    dHdl = pd.concat([gmx.extract_dHdl(filename, T=300)\n",
    "                      for filename in dataset['data']['Coulomb']])\n",
    "\n",
    "    return dHdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to spread the function out to have the execution time per line. For example, the load_benzene() is for five compressed xvg files like:\n",
    "\n",
    "```\n",
    "\n",
    "    $ ls ~/venv/alchemlyb/lib/python2.7/site-packages/alchemtest-0.2.0rc1+6.g2f0f9a1-py2.7.egg/alchemtest/gmx/benzene/Coulomb/* -alh\n",
    "    ~/venv/alchemlyb/lib/python2.7/site-packages/alchemtest-0.2.0rc1+6.g2f0f9a1-py2.7.egg/alchemtest/gmx/benzene/Coulomb/0000:\n",
    "    -rw------- 1 lee212 lee212 106K Feb  3 22:51 dhdl.xvg.bz2\n",
    "\n",
    "    ~/venv/alchemlyb/lib/python2.7/site-packages/alchemtest-0.2.0rc1+6.g2f0f9a1-py2.7.egg/alchemtest/gmx/benzene/Coulomb/0250:\n",
    "    -rw------- 1 lee212 lee212 109K Feb  3 22:51 dhdl.xvg.bz2\n",
    "\n",
    "    ~/venv/alchemlyb/lib/python2.7/site-packages/alchemtest-0.2.0rc1+6.g2f0f9a1-py2.7.egg/alchemtest/gmx/benzene/Coulomb/0500:\n",
    "    -rw------- 1 lee212 lee212 107K Feb  3 22:51 dhdl.xvg.bz2\n",
    "\n",
    "    ~/venv/alchemlyb/lib/python2.7/site-packages/alchemtest-0.2.0rc1+6.g2f0f9a1-py2.7.egg/alchemtest/gmx/benzene/Coulomb/0750:\n",
    "    -rw------- 1 lee212 lee212 112K Feb  3 22:51 dhdl.xvg.bz2\n",
    "\n",
    "    ~/venv/alchemlyb/lib/python2.7/site-packages/alchemtest-0.2.0rc1+6.g2f0f9a1-py2.7.egg/alchemtest/gmx/benzene/Coulomb/1000:\n",
    "    -rw------- 1 lee212 lee212 110K Feb  3 22:51 dhdl.xvg.bz2\n",
    "```\n",
    "\n",
    "Five files with the size of about 100k bytes where the total size is around 1.5MB (300k * 5 files) when it's uncompressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is same but with multiple lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import alchemtest.gmx\n",
    "from alchemlyb.parsing import gmx\n",
    "from alchemlyb.estimators import TI\n",
    "\n",
    "@profile\n",
    "def gmx_benzene_coul_dHdl():\n",
    "    dataset = alchemtest.gmx.load_benzene()\n",
    "    d1 = gmx.extract_dHdl(bz['data']['Coulomb'][0], T=300)\n",
    "    d2 = gmx.extract_dHdl(bz['data']['Coulomb'][1], T=300)\n",
    "    d3 = gmx.extract_dHdl(bz['data']['Coulomb'][2], T=300)\n",
    "    d4 = gmx.extract_dHdl(bz['data']['Coulomb'][3], T=300)\n",
    "    d5 = gmx.extract_dHdl(bz['data']['Coulomb'][4], T=300)\n",
    "    dHdl = pd.concat([d1, d2, d3, d4, d5])\n",
    "    TI().fit(dHdl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line profiler for gmx_benzene_coul_dHdl()\n",
    "\n",
    "I used [line_profiler](https://github.com/rkern/line_profiler) and the result is followed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Total time: 0.518848 s\n",
    "File: profile_ti_estimator.py\n",
    "Function: gmx_benzene_coul_dHdl at line 6\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     6                                           @profile\n",
    "     7                                           def gmx_benzene_coul_dHdl():\n",
    "     8         1      19848.0  19848.0      3.8      dataset = alchemtest.gmx.load_benzene()\n",
    "     9         1     102725.0 102725.0     19.8      d1 = gmx.extract_dHdl(bz['data']['Coulomb'][0], T=300)\n",
    "    10         1      92709.0  92709.0     17.9      d2 = gmx.extract_dHdl(bz['data']['Coulomb'][1], T=300)\n",
    "    11         1      90751.0  90751.0     17.5      d3 = gmx.extract_dHdl(bz['data']['Coulomb'][2], T=300)\n",
    "    12         1      91756.0  91756.0     17.7      d4 = gmx.extract_dHdl(bz['data']['Coulomb'][3], T=300)\n",
    "    13         1      93399.0  93399.0     18.0      d5 = gmx.extract_dHdl(bz['data']['Coulomb'][4], T=300)\n",
    "    14         1       6062.0   6062.0      1.2      dHdl = pd.concat([d1, d2, d3, d4, d5])\n",
    "    15         1      21594.0  21594.0      4.2      TI().fit(dHdl)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas concat is inexpensive and let's look at the extract_dHdl() function where 90% of time is consumed.\n",
    "\n",
    "Note that the Time value is microsecond in the above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract_dHdl()  line-by-line profiling\n",
    "\n",
    "The example here is parsing GROMACS xvg files by [parsing module, src/alchemlyb/parsing/gmx.py](https://github.com/alchemistry/alchemlyb/blob/master/src/alchemlyb/parsing/gmx.py#L112), and it indicates the file I/O is expensive where _extract_dataframe() is called (see line 131)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Timer unit: 1e-06 s\n",
    "\n",
    "Total time: 0.322493 s\n",
    "File: gmx.py\n",
    "Function: extract_dHdl at line 111\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "   111                                           @profile\n",
    "   112                                           def extract_dHdl(xvg, T):\n",
    "   113                                               \"\"\"Return gradients `dH/dl` from a Hamiltonian differences XVG file.\n",
    "   114                                           \n",
    "   115                                               Parameters\n",
    "   116                                               ----------\n",
    "   117                                               xvg : str\n",
    "   118                                                   Path to XVG file to extract data from.\n",
    "   119                                           \n",
    "   120                                               Returns\n",
    "   121                                               -------\n",
    "   122                                               dH/dl : Series\n",
    "   123                                                   dH/dl as a function of time for this lambda window.\n",
    "   124                                           \n",
    "   125                                               \"\"\"\n",
    "   126         5         10.0      2.0      0.0      beta = 1/(k_b * T)\n",
    "   127                                           \n",
    "   128         5      53664.0  10732.8     16.6      state, lambdas, statevec = _extract_state(xvg)\n",
    "   129                                           \n",
    "   130                                               # extract a DataFrame from XVG data\n",
    "   131         5     205188.0  41037.6     63.6      df = _extract_dataframe(xvg)\n",
    "   132                                           \n",
    "   133         5       1154.0    230.8      0.4      times = df[df.columns[0]]\n",
    "   134                                           \n",
    "   135                                               # want to grab only dH/dl columns\n",
    "   136         5          5.0      1.0      0.0      dHcols = []\n",
    "   137        10          8.0      0.8      0.0      for l in lambdas:\n",
    "   138        45        274.0      6.1      0.1          dHcols.extend([col for col in df.columns if (l in col)])\n",
    "   139                                           \n",
    "   140         5       7797.0   1559.4      2.4      dHdl = df[dHcols]\n",
    "   141                                           \n",
    "   142                                               # make dimensionless\n",
    "   143         5      14897.0   2979.4      4.6      dHdl = beta * dHdl\n",
    "   144                                           \n",
    "   145                                               # rename columns to not include the word 'lambda', since we use this for\n",
    "   146                                               # index below\n",
    "   147        10         45.0      4.5      0.0      cols = [l.split('-')[0] for l in lambdas]\n",
    "   148                                           \n",
    "   149         5        186.0     37.2      0.1      dHdl = pd.DataFrame(dHdl.values, columns=cols,\n",
    "   150         5       2377.0    475.4      0.7                          index=pd.Float64Index(times.values, name='time'))\n",
    "   151                                           \n",
    "   152                                               # create columns for each lambda, indicating state each row sampled from\n",
    "   153                                               # if state is None run as expanded ensemble data or REX\n",
    "   154         5          5.0      1.0      0.0      if state is None:\n",
    "   155                                                   # if thermodynamic state is specified map thermodynamic\n",
    "   156                                                   # state data to lambda values, else (for REX)\n",
    "   157                                                   # define state based on the legend\n",
    "   158                                                   if 'Thermodynamic state' in df:\n",
    "   159                                                       ts_index = df.columns.get_loc('Thermodynamic state')\n",
    "   160                                                       thermo_state = df[df.columns[ts_index]]\n",
    "   161                                                       for i, l in enumerate(lambdas):\n",
    "   162                                                           v = []\n",
    "   163                                                           for t in thermo_state:\n",
    "   164                                                               v.append(statevec[int(t)][i])\n",
    "   165                                                           dHdl[l] = v\n",
    "   166                                                   else:\n",
    "   167                                                       state_legend = _extract_legend(xvg)\n",
    "   168                                                       for i, l in enumerate(state_legend):\n",
    "   169                                                           dHdl[l] = state_legend[l]\n",
    "   170                                               else:\n",
    "   171        10         25.0      2.5      0.0          for i, l in enumerate(lambdas):\n",
    "   172         5          5.0      1.0      0.0              try:\n",
    "   173         5         13.0      2.6      0.0                  dHdl[l] = statevec[i]\n",
    "   174         5          7.0      1.4      0.0              except TypeError:\n",
    "   175         5       4646.0    929.2      1.4                  dHdl[l] = statevec\n",
    "   176                                           \n",
    "   177                                               # set up new multi-index\n",
    "   178         5         17.0      3.4      0.0      newind = ['time'] + lambdas\n",
    "   179         5      31846.0   6369.2      9.9      dHdl= dHdl.reset_index().set_index(newind)\n",
    "   180                                           \n",
    "   181         5        322.0     64.4      0.1      dHdl.name='dH/dl'\n",
    "   182                                           \n",
    "   183         5          2.0      0.4      0.0      return dHdl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _extract_dataframe() line-by-line profiling\n",
    "\n",
    "Another profiling for the internal function to read a file is followed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Timer unit: 1e-06 s\n",
    "\n",
    "Total time: 0.245378 s\n",
    "File: gmx.py\n",
    "Function: _extract_dataframe at line 226\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "   226                                           @profile\n",
    "   227                                           def _extract_dataframe(xvg):\n",
    "   228                                               \"\"\"Extract a DataFrame from XVG data.\n",
    "   229\n",
    "   230                                               \"\"\"\n",
    "   231         5       2505.0    501.0      1.0      with anyopen(xvg, 'r') as f:\n",
    "   232         5          9.0      1.8      0.0          names = []\n",
    "   233         5          4.0      0.8      0.0          rows = []\n",
    "   234     20160      82187.0      4.1     33.5          for line in f:\n",
    "   235     20155      12877.0      0.6      5.2              line = line.strip()\n",
    "   236     20155      12271.0      0.6      5.0              if len(line) == 0:\n",
    "   237                                                           continue\n",
    "   238\n",
    "   239     20155      12537.0      0.6      5.1              if \"label\" in line and \"xaxis\" in line:\n",
    "   240         5         10.0      2.0      0.0                  xaxis = line.split('\"')[-2]\n",
    "   241\n",
    "   242     20155      14119.0      0.7      5.8              if line.startswith(\"@ s\") and \"subtitle\" not in line:\n",
    "   243        35         51.0      1.5      0.0                  name = line.split(\"legend \")[-1].replace('\"','').strip()\n",
    "   244        35         23.0      0.7      0.0                  names.append(name)\n",
    "   245\n",
    "   246                                                       # should catch non-numeric lines so we don't proceed in parsing\n",
    "   247                                                       # here\n",
    "   248     20155      13943.0      0.7      5.7              if line.startswith(('#', '@')):\n",
    "   249       150         82.0      0.5      0.0                  continue\n",
    "   250\n",
    "   251     20005      13550.0      0.7      5.5              if line.startswith('&'):  #pragma: no cover\n",
    "   252                                                           raise NotImplementedError('{}: Multi-data not supported,'\n",
    "   253                                                                                     'only simple NXY format.'.format(xvg))\n",
    "   254                                                       # parse line as floats\n",
    "   255     20005      50123.0      2.5     20.4              row = map(float, line.split())\n",
    "   256     20005      13916.0      0.7      5.7              rows.append(row)\n",
    "   257\n",
    "   258         5          5.0      1.0      0.0      cols = [xaxis]\n",
    "   259         5          9.0      1.8      0.0      cols.extend(names)\n",
    "   260\n",
    "   261         5      17157.0   3431.4      7.0      return pd.DataFrame(rows, columns=cols)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There might be some tuning points in this function for parsing headers and rendering floating values since gmx.extract_dHdl() function consumes most of the execution time here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FYI, the content of a xvg file looks like:\n",
    "\n",
    "```\n",
    "# This file was created Tue Mar 21 13:50:08 2017\n",
    "# Created by:\n",
    "#                  :-) GROMACS - gmx energy, VERSION 5.1.4 (-:\n",
    "# \n",
    "# Executable:   /nfs/packages/opt/Linux_x86_64/gromacs/5.1.4/cuda7.5/gnu-4.8/avx/bin/gmx\n",
    "# Data prefix:  /nfs/packages/opt/Linux_x86_64/gromacs/5.1.4/cuda7.5/gnu-4.8/avx\n",
    "# Command line:\n",
    "#   gmx energy -odh /nfs/homes4/dldotson/Desktop/ikenney/Coulomb/0000/dhdl.xvg -s /nfs/homes4/dldotson/Desktop/ikenney/Coulomb/000\n",
    "0/md.tpr -f /nfs/homes4/dldotson/Desktop/ikenney/Coulomb/0000/md.edr\n",
    "# gmx energy is part of G R O M A C S:\n",
    "#\n",
    "# Gromacs Runs On Most of All Computer Systems\n",
    "#\n",
    "@    title \"dH/d\\xl\\f{} and \\xD\\f{}H\"\n",
    "@    xaxis  label \"Time (ps)\"\n",
    "@    yaxis  label \"dH/d\\xl\\f{} and \\xD\\f{}H (kJ/mol [\\xl\\f{}]\\S-1\\N)\"\n",
    "@TYPE xy\n",
    "@ subtitle \"T = 300 (K) \\xl\\f{} state 0: fep-lambda = 0.0000\"\n",
    "@ view 0.15, 0.15, 0.75, 0.85\n",
    "@ legend on\n",
    "@ legend box on\n",
    "@ legend loctype view\n",
    "@ legend 0.78, 0.8\n",
    "@ legend length 2\n",
    "@ s0 legend \"dH/d\\xl\\f{} fep-lambda = 0.0000\"\n",
    "@ s1 legend \"\\xD\\f{}H \\xl\\f{} to 0.0000\"\n",
    "@ s2 legend \"\\xD\\f{}H \\xl\\f{} to 0.2500\"\n",
    "@ s3 legend \"\\xD\\f{}H \\xl\\f{} to 0.5000\"\n",
    "@ s4 legend \"\\xD\\f{}H \\xl\\f{} to 0.7500\"\n",
    "@ s5 legend \"\\xD\\f{}H \\xl\\f{} to 1.0000\"\n",
    "@ s6 legend \"pV (kJ/mol)\"\n",
    "0.0000  33.399342 0.0000000 8.3498354 16.699671 25.049507 33.399342 0.77155721\n",
    "10.0000  23.026176 0.0000000 5.7565441 11.513088 17.269632 23.026176 0.77036208\n",
    "20.0000  13.227966 0.0000000 3.3069916 6.6139832 9.9209747 13.227966 0.75064653\n",
    "30.0000  12.670597 0.0000000 3.1676493 6.3352985 9.5029478 12.670597 0.77252579\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory profiler for gmx_benzene_coul_dHdl()\n",
    "\n",
    "Each xvg file is about 300kb size when it's uncompressed, and the memory consumption here is around 5MB for five 300kb xvg files.\n",
    "```\n",
    "Line #    Mem usage    Increment   Line Contents\n",
    "================================================\n",
    "     6   78.496 MiB   78.496 MiB   @profile\n",
    "     7                             def gmx_benzene_coul_dHdl():\n",
    "     8   78.508 MiB    0.012 MiB       dataset = alchemtest.gmx.load_benzene()\n",
    "     9   83.871 MiB    5.363 MiB       d1 = gmx.extract_dHdl(dataset['data']['Coulomb'][0], T=300)\n",
    "    10   87.473 MiB    3.602 MiB       d2 = gmx.extract_dHdl(dataset['data']['Coulomb'][1], T=300)\n",
    "    11   87.484 MiB    0.012 MiB       d3 = gmx.extract_dHdl(dataset['data']['Coulomb'][2], T=300)\n",
    "    12   87.496 MiB    0.012 MiB       d4 = gmx.extract_dHdl(dataset['data']['Coulomb'][3], T=300)\n",
    "    13   87.496 MiB    0.000 MiB       d5 = gmx.extract_dHdl(dataset['data']['Coulomb'][4], T=300)\n",
    "    14   87.512 MiB    0.016 MiB       dHdl = pd.concat([d1, d2, d3, d4, d5])\n",
    "    15   87.684 MiB    0.172 MiB       TI().fit(dHdl)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprosessing - statistical_inefficiency()\n",
    "\n",
    "Profiling for preprocessing is also available, and this is just to explore other optimization opportunities.\n",
    "The code snippet below is based on the test script here: https://github.com/alchemistry/alchemlyb/blob/master/src/alchemlyb/tests/test_preprocessing.py\n",
    "\n",
    "```\n",
    "Timer unit: 1e-06 s\n",
    "\n",
    "Total time: 0.13373 s\n",
    "File: alchemlyb_preprocessing.py\n",
    "Function: main at line 14\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "    14                                           @profile\n",
    "    15                                           def main():\n",
    "    16         1      68344.0  68344.0     51.1      d = gmx_benzene_dHdl()\n",
    "    17         1      11295.0  11295.0      8.4      slicing(d, lower=1000, upper=34000, step=5)\n",
    "    18         1      22904.0  22904.0     17.1      statistical_inefficiency(d, series=d.iloc[:, 0], conservative=True)\n",
    "    19         1      31187.0  31187.0     23.3      statistical_inefficiency(d, series=d.iloc[:, 0], conservative=False)\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### statistical_inefficiency() line-by-line profiling\n",
    "[This function](https://github.com/alchemistry/alchemlyb/blob/master/src/alchemlyb/preprocessing/subsampling.py) uses pymbar and the profiling results is:\n",
    "\n",
    "```\n",
    "Total time: 0.054 s\n",
    "File: subsampling.py\n",
    "Function: statistical_inefficiency at line 55\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "    55                                           @profile\n",
    "    56                                           def statistical_inefficiency(df, series=None, lower=None, upper=None, step=None,\n",
    "    57                                                                        conservative=True):\n",
    "   ... (skipped description) ...\n",
    "   118         2      12560.0   6280.0     23.3      if _check_multiple_times(df):\n",
    "   119                                                   raise KeyError(\"Duplicate time values found; statistical inefficiency \"\n",
    "   120                                                                  \"only works on a single, contiguous, \"\n",
    "   121                                                                  \"and sorted timeseries.\")\n",
    "   122                                           \n",
    "   123         2       3767.0   1883.5      7.0      if not _check_sorted(df):\n",
    "   124                                                   raise KeyError(\"Statistical inefficiency only works as expected if \"\n",
    "   125                                                                  \"values are sorted by time, increasing.\")\n",
    "   126                                           \n",
    "   127         2          3.0      1.5      0.0      if series is not None:\n",
    "   128         2      14838.0   7419.0     27.5          series = slicing(series, lower=lower, upper=upper, step=step)\n",
    "   129                                           \n",
    "   130         2         22.0     11.0      0.0          if (len(series) != len(df) or\n",
    "   131         2      11881.0   5940.5     22.0              not all(series.reset_index()['time'] == df.reset_index()['time'])):\n",
    "   132                                                       raise ValueError(\"series and data must be sampled at the same times\")\n",
    "   133                                           \n",
    "   134                                                   # calculate statistical inefficiency of series (could use fft=True but needs test)\n",
    "   135         2        760.0    380.0      1.4          statinef  = statisticalInefficiency(series, fast=False)\n",
    "   136                                           \n",
    "   137                                                   # use the subsampleCorrelatedData function to get the subsample index\n",
    "   138         2          2.0      1.0      0.0          indices = subsampleCorrelatedData(series, g=statinef,\n",
    "   139         2       8703.0   4351.5     16.1                                            conservative=conservative)\n",
    "   140         2       1462.0    731.0      2.7          df = df.iloc[indices]\n",
    "   141                                               else:\n",
    "   142                                                   df = slicing(df, lower=lower, upper=upper, step=step)\n",
    "   143                                           \n",
    "   144         2          2.0      1.0      0.0      return df\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "If we think this is useful, we may run additional experiments for scaling tests e.g. with a large number of xvg files and different file sizes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
